<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>> 7 Inferência ativa em tempo discreto | Inferencia Ativa</title>
  <meta name="description" content="O Princípio da Energia Livre na Mente, Cérebro e Comportamento." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="> 7 Inferência ativa em tempo discreto | Inferencia Ativa" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/Figura_1_1.png" />
  <meta property="og:description" content="O Princípio da Energia Livre na Mente, Cérebro e Comportamento." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="> 7 Inferência ativa em tempo discreto | Inferencia Ativa" />
  
  <meta name="twitter:description" content="O Princípio da Energia Livre na Mente, Cérebro e Comportamento." />
  <meta name="twitter:image" content="/images/Figura_1_1.png" />

<meta name="author" content="Thomas Parr, Giovanni Pezzulo, and Karl J. Friston" />


<meta name="date" content="2022-07-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="uma-receita-para-projetar-modelos-de-inferência-ativos.html"/>
<link rel="next" href="inferência-ativa-em-tempo-contínuo-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inferencia Ativa</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Conteúdo</a></li>
<li class="chapter" data-level="" data-path="prefácio.html"><a href="prefácio.html"><i class="fa fa-check"></i>Prefácio</a></li>
<li class="chapter" data-level="1" data-path="visão-geral.html"><a href="visão-geral.html"><i class="fa fa-check"></i><b>1</b> Visão Geral</a>
<ul>
<li class="chapter" data-level="1.1" data-path="visão-geral.html"><a href="visão-geral.html#introdução"><i class="fa fa-check"></i><b>1.1</b> Introdução</a></li>
<li class="chapter" data-level="1.2" data-path="visão-geral.html"><a href="visão-geral.html#como-os-organismos-vivos-persistem-e-agem-adaptativamente"><i class="fa fa-check"></i><b>1.2</b> Como os Organismos Vivos Persistem e Agem Adaptativamente?</a></li>
<li class="chapter" data-level="1.3" data-path="visão-geral.html"><a href="visão-geral.html#inferência-ativa-comportamento-a-partir-dos-primeiros-princípios"><i class="fa fa-check"></i><b>1.3</b> Inferência Ativa: Comportamento a partir dos Primeiros Princípios</a></li>
<li class="chapter" data-level="1.4" data-path="visão-geral.html"><a href="visão-geral.html#estrutura-do-livro"><i class="fa fa-check"></i><b>1.4</b> Estrutura do Livro</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="visão-geral.html"><a href="visão-geral.html#parte-1-inferência-ativa-na-teoria"><i class="fa fa-check"></i><b>1.4.1</b> Parte 1: Inferência Ativa na Teoria</a></li>
<li class="chapter" data-level="1.4.2" data-path="visão-geral.html"><a href="visão-geral.html#parte-2-inferência-ativa-na-prática"><i class="fa fa-check"></i><b>1.4.2</b> Parte 2: Inferência Ativa na Prática</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="visão-geral.html"><a href="visão-geral.html#resumo"><i class="fa fa-check"></i><b>1.5</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html"><i class="fa fa-check"></i><b>2</b> O Caminho de baixo para a Inferência Ativa</a>
<ul>
<li class="chapter" data-level="2.1" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#introdução-1"><i class="fa fa-check"></i><b>2.1</b> Introdução</a></li>
<li class="chapter" data-level="2.2" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#percepção-como-inferência"><i class="fa fa-check"></i><b>2.2</b> Percepção como Inferência</a></li>
<li class="chapter" data-level="2.3" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#inferência-biológica-e-otimização"><i class="fa fa-check"></i><b>2.3</b> Inferência Biológica e Otimização</a></li>
<li class="chapter" data-level="2.4" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#ação-como-inferência"><i class="fa fa-check"></i><b>2.4</b> Ação como Inferência</a></li>
<li class="chapter" data-level="2.5" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#minimizando-a-discrepância-entre-o-modelo-e-o-mundo"><i class="fa fa-check"></i><b>2.5</b> Minimizando a discrepância entre o modelo e o mundo</a></li>
<li class="chapter" data-level="2.6" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#minimizando-a-energia-livre-variacional"><i class="fa fa-check"></i><b>2.6</b> Minimizando a Energia Livre Variacional</a></li>
<li class="chapter" data-level="2.7" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#energia-livre-esperada-e-planejamento-como-inferência"><i class="fa fa-check"></i><b>2.7</b> Energia Livre Esperada e Planejamento como Inferência</a></li>
<li class="chapter" data-level="2.8" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#o-que-é-energia-livre-esperada"><i class="fa fa-check"></i><b>2.8</b> O que é energia livre esperada?</a></li>
<li class="chapter" data-level="2.9" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#no-final-da-estrada-baixa"><i class="fa fa-check"></i><b>2.9</b> No final da estrada baixa</a></li>
<li class="chapter" data-level="2.10" data-path="o-caminho-de-baixo-para-a-inferência-ativa.html"><a href="o-caminho-de-baixo-para-a-inferência-ativa.html#resumo-1"><i class="fa fa-check"></i><b>2.10</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html"><i class="fa fa-check"></i><b>3</b> O caminho para a inferência ativa</a>
<ul>
<li class="chapter" data-level="3.1" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#introdução-2"><i class="fa fa-check"></i><b>3.1</b> Introdução</a></li>
<li class="chapter" data-level="3.2" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#envoltórios-de-markov"><i class="fa fa-check"></i><b>3.2</b> Envoltórios de Markov</a></li>
<li class="chapter" data-level="3.3" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#minimização-de-surpresa-e-auto-evidência"><i class="fa fa-check"></i><b>3.3</b> Minimização de surpresa e auto-evidência</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#minimização-de-surpresa-como-um-princípio-hamiltoniano-de-menor-ação"><i class="fa fa-check"></i><b>3.3.1</b> Minimização de surpresa como um princípio hamiltoniano de menor ação</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#relações-entre-inferência-cognição-e-dinâmica-estocástica"><i class="fa fa-check"></i><b>3.4</b> Relações entre Inferência, Cognição e Dinâmica Estocástica</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#energia-livre-variacional-evidência-modelo-e-surpresa"><i class="fa fa-check"></i><b>3.4.1</b> Energia Livre Variacional, Evidência Modelo e Surpresa</a></li>
<li class="chapter" data-level="3.4.2" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#energia-livre-esperada-e-inferência-da-trajetória-mais-provável"><i class="fa fa-check"></i><b>3.4.2</b> Energia Livre Esperada e Inferência da Trajetória Mais Provável</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#inferência-ativa-uma-nova-base-para-entender-o-comportamento-e-a-cognição"><i class="fa fa-check"></i><b>3.5</b> Inferência ativa: uma nova base para entender o comportamento e a cognição</a></li>
<li class="chapter" data-level="3.6" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#modelos-políticas-e-trajetórias"><i class="fa fa-check"></i><b>3.6</b> Modelos, Políticas e Trajetórias</a></li>
<li class="chapter" data-level="3.7" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#reconciliação-das-teorias-enativa-cibernética-e-preditiva-sob-inferência-ativa"><i class="fa fa-check"></i><b>3.7</b> Reconciliação das Teorias Enativa, Cibernética e Preditiva sob Inferência Ativa</a></li>
<li class="chapter" data-level="3.8" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#inferência-ativa-da-emergência-da-vida-para-a-atuação"><i class="fa fa-check"></i><b>3.8</b> Inferência Ativa, da Emergência da Vida para a atuação</a></li>
<li class="chapter" data-level="3.9" data-path="o-caminho-para-a-inferência-ativa.html"><a href="o-caminho-para-a-inferência-ativa.html#resumo-2"><i class="fa fa-check"></i><b>3.9</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html"><i class="fa fa-check"></i><b>4</b> Os Modelos Geradores de Inferência Ativa</a>
<ul>
<li class="chapter" data-level="4.1" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#introdução-3"><i class="fa fa-check"></i><b>4.1</b> Introdução</a></li>
<li class="chapter" data-level="4.2" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#da-inferência-bayesiana-à-energia-livre"><i class="fa fa-check"></i><b>4.2</b> Da Inferência Bayesiana à Energia Livre</a></li>
<li class="chapter" data-level="4.3" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#modelos-geradores"><i class="fa fa-check"></i><b>4.3</b> Modelos Geradores</a></li>
<li class="chapter" data-level="4.4" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#inferência-ativa-em-tempo-discreto"><i class="fa fa-check"></i><b>4.4</b> Inferência ativa em tempo discreto</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#processos-de-decisão-markov-parcialmente-observáveis"><i class="fa fa-check"></i><b>4.4.1</b> Processos de Decisão Markov Parcialmente Observáveis</a></li>
<li class="chapter" data-level="4.4.2" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#inferência-ativa-em-um-pomdp"><i class="fa fa-check"></i><b>4.4.2</b> Inferência ativa em um POMDP</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#inferência-ativa-em-tempo-contínuo"><i class="fa fa-check"></i><b>4.5</b> Inferência Ativa em Tempo Contínuo</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#um-modelo-generativo-para-codificação-preditiva"><i class="fa fa-check"></i><b>4.5.1</b> Um modelo generativo para codificação preditiva</a></li>
<li class="chapter" data-level="4.5.2" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#inferência-ativa-como-codificação-preditiva-com-reflexos-motores"><i class="fa fa-check"></i><b>4.5.2</b> Inferência Ativa como Codificação Preditiva com Reflexos Motores</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="os-modelos-geradores-de-inferência-ativa.html"><a href="os-modelos-geradores-de-inferência-ativa.html#resumo-3"><i class="fa fa-check"></i><b>4.6</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html"><i class="fa fa-check"></i><b>5</b> Passagem de Mensagens e Neurobiologia</a>
<ul>
<li class="chapter" data-level="5.1" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#introdução-4"><i class="fa fa-check"></i><b>5.1</b> Introdução</a></li>
<li class="chapter" data-level="5.2" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#microcircuitos-e-mensagens"><i class="fa fa-check"></i><b>5.2</b> Microcircuitos e Mensagens</a></li>
<li class="chapter" data-level="5.3" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#comandos-do-motor"><i class="fa fa-check"></i><b>5.3</b> Comandos do Motor</a></li>
<li class="chapter" data-level="5.4" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#estruturas-subcorticais"><i class="fa fa-check"></i><b>5.4</b> Estruturas Subcorticais</a></li>
<li class="chapter" data-level="5.5" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#neuromodulação-e-aprendizagem"><i class="fa fa-check"></i><b>5.5</b> Neuromodulação e Aprendizagem</a></li>
<li class="chapter" data-level="5.6" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#hierarquias-contínuas-e-discretas"><i class="fa fa-check"></i><b>5.6</b> Hierarquias Contínuas e Discretas</a></li>
<li class="chapter" data-level="5.7" data-path="passagem-de-mensagens-e-neurobiologia.html"><a href="passagem-de-mensagens-e-neurobiologia.html#resumo-4"><i class="fa fa-check"></i><b>5.7</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><i class="fa fa-check"></i><b>6</b> Uma receita para projetar modelos de inferência ativos</a>
<ul>
<li class="chapter" data-level="6.1" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#introdução-5"><i class="fa fa-check"></i><b>6.1</b> Introdução</a></li>
<li class="chapter" data-level="6.2" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#projetando-um-modelo-de-inferência-ativo-uma-receita-em-quatro-etapas"><i class="fa fa-check"></i><b>6.2</b> Projetando um modelo de inferência ativo: uma receita em quatro etapas</a></li>
<li class="chapter" data-level="6.3" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#que-sistema-estamos-modelando"><i class="fa fa-check"></i><b>6.3</b> Que sistema estamos modelando?</a></li>
<li class="chapter" data-level="6.4" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#qual-é-a-forma-mais-adequada-para-o-modelo-generativo"><i class="fa fa-check"></i><b>6.4</b> Qual é a forma mais adequada para o modelo generativo?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#variáveis-discretas-ou-contínuas-ou-ambas"><i class="fa fa-check"></i><b>6.4.1</b> Variáveis Discretas ou Contínuas (ou Ambas)?</a></li>
<li class="chapter" data-level="6.4.2" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#escalas-de-tempo-de-inferência-modelos-rasos-versus-modelos-hierárquicos"><i class="fa fa-check"></i><b>6.4.2</b> Escalas de tempo de inferência: modelos rasos versus modelos hierárquicos</a></li>
<li class="chapter" data-level="6.4.3" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#profundidade-temporal-de-inferência-e-planejamento"><i class="fa fa-check"></i><b>6.4.3</b> Profundidade Temporal de Inferência e Planejamento</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#como-configurar-o-modelo-generativo"><i class="fa fa-check"></i><b>6.5</b> Como configurar o modelo generativo?</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#configurando-as-variáveis-do-modelo-gerador"><i class="fa fa-check"></i><b>6.5.1</b> Configurando as Variáveis do Modelo Gerador</a></li>
<li class="chapter" data-level="6.5.2" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#quais-partes-do-modelo-generativo-são-fixas-e-o-que-é-aprendido"><i class="fa fa-check"></i><b>6.5.2</b> Quais partes do modelo generativo são fixas e o que é aprendido?|</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#configurando-o-processo-gerador"><i class="fa fa-check"></i><b>6.6</b> Configurando o Processo Gerador</a></li>
<li class="chapter" data-level="6.7" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#simulando-visualizando-analisando-e-ajustando-dados-usando-inferência-ativa"><i class="fa fa-check"></i><b>6.7</b> Simulando, Visualizando, Analisando e Ajustando Dados Usando Inferência Ativa</a></li>
<li class="chapter" data-level="6.8" data-path="uma-receita-para-projetar-modelos-de-inferência-ativos.html"><a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html#resumo-5"><i class="fa fa-check"></i><b>6.8</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html"><i class="fa fa-check"></i><b>7</b> Inferência ativa em tempo discreto</a>
<ul>
<li class="chapter" data-level="7.1" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#introdução-6"><i class="fa fa-check"></i><b>7.1</b> Introdução</a></li>
<li class="chapter" data-level="7.2" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#processamento-perceptivo"><i class="fa fa-check"></i><b>7.2</b> Processamento Perceptivo</a></li>
<li class="chapter" data-level="7.3" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#tomada-de-decisão-e-planejamento-como-inferência"><i class="fa fa-check"></i><b>7.3</b> Tomada de decisão e planejamento como inferência</a></li>
<li class="chapter" data-level="7.4" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#busca-de-informações"><i class="fa fa-check"></i><b>7.4</b> Busca de informações</a></li>
<li class="chapter" data-level="7.5" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#aprendizado-e-novidade"><i class="fa fa-check"></i><b>7.5</b> Aprendizado e Novidade</a></li>
<li class="chapter" data-level="7.6" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#inferência-hierárquica-ou-profunda"><i class="fa fa-check"></i><b>7.6</b> Inferência Hierárquica ou Profunda</a></li>
<li class="chapter" data-level="7.7" data-path="inferência-ativa-em-tempo-discreto-1.html"><a href="inferência-ativa-em-tempo-discreto-1.html#resumo-6"><i class="fa fa-check"></i><b>7.7</b> Resumo</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inferência-ativa-em-tempo-contínuo-1.html"><a href="inferência-ativa-em-tempo-contínuo-1.html"><i class="fa fa-check"></i><b>8</b> Inferência ativa em tempo contínuo</a>
<ul>
<li class="chapter" data-level="8.1" data-path="inferência-ativa-em-tempo-contínuo-1.html"><a href="inferência-ativa-em-tempo-contínuo-1.html#introdução-7"><i class="fa fa-check"></i><b>8.1</b> Introdução</a></li>
<li class="chapter" data-level="8.2" data-path="inferência-ativa-em-tempo-contínuo-1.html"><a href="inferência-ativa-em-tempo-contínuo-1.html#controles-de-movimento"><i class="fa fa-check"></i><b>8.2</b> Controles de movimento</a></li>
<li class="chapter" data-level="8.3" data-path="inferência-ativa-em-tempo-contínuo-1.html"><a href="inferência-ativa-em-tempo-contínuo-1.html#sistemas-dinâmicos"><i class="fa fa-check"></i><b>8.3</b> Sistemas Dinâmicos</a></li>
<li class="chapter" data-level="8.4" data-path="inferência-ativa-em-tempo-contínuo-1.html"><a href="inferência-ativa-em-tempo-contínuo-1.html#sincronia-generalizada"><i class="fa fa-check"></i><b>8.4</b> Sincronia Generalizada</a></li>
<li class="chapter" data-level="8.5" data-path="inferência-ativa-em-tempo-contínuo-1.html"><a href="inferência-ativa-em-tempo-contínuo-1.html#modelos-híbridos-discretos-e-contínuos"><i class="fa fa-check"></i><b>8.5</b> Modelos Híbridos (Discretos e Contínuos)</a></li>
</ul></li>
<li class="appendix"><span><b>Apêndice</b></span></li>
<li class="chapter" data-level="A" data-path="bases-matemáticas.html"><a href="bases-matemáticas.html"><i class="fa fa-check"></i><b>A</b> Bases Matemáticas</a>
<ul>
<li class="chapter" data-level="A.1" data-path="bases-matemáticas.html"><a href="bases-matemáticas.html#introdução-8"><i class="fa fa-check"></i><b>A.1</b> Introdução</a></li>
<li class="chapter" data-level="A.2" data-path="bases-matemáticas.html"><a href="bases-matemáticas.html#álgebra-linear"><i class="fa fa-check"></i><b>A.2</b> Álgebra Linear</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="bases-matemáticas.html"><a href="bases-matemáticas.html#o-básico"><i class="fa fa-check"></i><b>A.2.1</b> O básico</a></li>
<li class="chapter" data-level="A.2.2" data-path="bases-matemáticas.html"><a href="bases-matemáticas.html#derivadas"><i class="fa fa-check"></i><b>A.2.2</b> Derivadas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="as-equações-da-inferência-ativa.html"><a href="as-equações-da-inferência-ativa.html"><i class="fa fa-check"></i><b>B</b> As equações da inferência ativa</a></li>
<li class="chapter" data-level="" data-path="dicionário.html"><a href="dicionário.html"><i class="fa fa-check"></i>Dicionário</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inferencia Ativa</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferência-ativa-em-tempo-discreto-1" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">> 7</span> Inferência ativa em tempo discreto<a href="inferência-ativa-em-tempo-discreto-1.html#inferência-ativa-em-tempo-discreto-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>O que eu não posso criar, eu não entendo. — Richard Feynman</p>
<div id="introdução-6" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Introdução<a href="inferência-ativa-em-tempo-discreto-1.html#introdução-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Até agora, discutimos os princípios da Inferência Ativa em um nível relativamente abstrato. Este capítulo trata de exemplos específicos — e como eles podem ser especificados em um cenário prático. Focamos em modelos de variáveis categóricas em tempo discreto. Por meio de uma série de exemplos, construídos em complexidade, ilustramos modelos de processamento perceptual, tomada de decisão, busca de informações, aprendizado e inferência hierárquica. Esses exemplos são escolhidos para destacar as propriedades emergentes da forma mais simples possível – incluindo fisiologia e comportamento mensuráveis – dos esquemas de Inferência Ativa.</p>
</div>
<div id="processamento-perceptivo" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Processamento Perceptivo<a href="inferência-ativa-em-tempo-discreto-1.html#processamento-perceptivo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Começamos considerando o processamento perceptual e a inversão do tipo de modelos de tempo discreto introduzidos no capítulo 4. Mais adiante neste capítulo, construímos um processo de decisão Markov parcialmente observável completo (POMDP). No entanto, começamos com um caso especial de um POMDP no qual podemos ignorar escolhas e comportamento: um modelo oculto de Markov (HMM), que pode ser usado para inferência perceptual de uma classificação sequencial e categórica (veja a figura 7.1). Para motivar isso, vamos recorrer a um exemplo simples. Imagine ouvir uma apresentação de uma pequena peça de música. A sequência de notas que são escritas na partitura pode ser pensada como estados ocultos (não observados), enquanto a sequência de notas que realmente ouvimos são os resultados (observáveis). Se o intérprete é um músico profissional, a correspondência entre os estados ocultos e os resultados pode ser muito próxima. No entanto, se for um amador, pode haver um grau adicional de estocasticidade no mapeamento (de probabilidade) da nota que deve ser tocada para a que é ouvida. Nesse cenário, ainda pode ser possível inferir qual nota deveria ter sido ouvida, dadas as crenças prévias sobre a probabilidade de cada nota ser precedida ou sucedida por outra.</p>
<div class="figure">
<img src="images/Figura_7_1.png" alt="" />
<p class="caption"><strong>Figura 7.1</strong> Este modelo oculto de Markov usa a mesma notação introduzida no capítulo 4 para expressar uma sequência de estados que evoluem ao longo do tempo. A cada vez, eles dão origem a um resultado observável (o). O estado em um momento depende apenas do estado no momento anterior (com essa dependência expressa em B). O primeiro estado na sequência tem probabilidade anterior D. A geração de resultados dos estados depende da distribuição de verossimilhança (A). Esta especificação de um HMM é genérica, com modelos generativos específicos dependendo de escolhas específicas para A, B e D.</p>
</div>
<p>O exemplo de escuta do músico amador pode ser formalizado da seguinte forma. Primeiro, decidimos com que confiabilidade nosso músico realmente toca a nota (resultado) que ela pretende (estado oculto). Podemos expressar isso através da matriz A, cujos elementos indicam a probabilidade de um resultado (linhas) dado um estado (colunas). Em nosso exemplo de brinquedo, definimos isso da seguinte forma:</p>
<p><span class="math display">\[ A = \frac{1}{10}\begin{bmatrix}
7 &amp; 1 &amp; 1 &amp; 1  \\
1 &amp; 7 &amp; 1 &amp; 1  \\
1 &amp; 1 &amp; 7 &amp; 1  \\
1 &amp; 1 &amp; 1 &amp; 7  \\
\end{bmatrix} \qquad\qquad\qquad (7.1) \]</span></p>
<p>Isso diz que 70% das vezes, nosso músico atinge a nota pretendida. Em seguida, especificamos as probabilidades de transição na matriz B, que explicam a probabilidade do próximo estado (linhas) dado o estado atual (colunas)</p>
<p><span class="math display">\[ B = \frac{1}{100}\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 97  \\
97 &amp; 1 &amp; 1 &amp; 1  \\
1 &amp; 97 &amp; 7 &amp; 1  \\
1 &amp; 1 &amp; 97 &amp; 1  \\
\end{bmatrix} \qquad\qquad\qquad (7.2) \]</span></p>
<p>Isso diz que há uma probabilidade de 97% de a primeira nota ser seguida pela segunda, a segunda pela terceira e assim por diante. Se sabemos que a sequência sempre começa com a primeira nota, definimos a probabilidade anterior:</p>
<p><span class="math display">\[ D = \begin{bmatrix}
1  \\
0 \\
0 \\
0 \\
\end{bmatrix} \qquad\qquad\qquad (7.3) \]</span></p>
<p>Juntas, as equações 7.1–7.3 especificam completamente o modelo generativo HMM mostrado na figura 7.1. Em outras palavras, eles fornecem uma descrição de nossas crenças sobre como a música que ouvimos é gerada por nosso músico amador. Usando a equação 4.12 e substituindo em nosso modelo generativo, podemos simular a dinâmica da atualização da crença Bayesiana induzida por uma sequência de resultados. Isso é mostrado na figura 7.2. Observe o aumento na confiança mostrado no gráfico superior esquerdo à medida que mais dados são acumulados ao longo do tempo, exceto para o terceiro passo de tempo, onde ocorreu um resultado inesperado. Esse resultado pode ser explicado de duas maneiras. Primeiro, pode ser que a nota pretendida realmente fosse uma nota incomum sob nossas crenças anteriores na equação 7.2. Isso é menos provável pela raridade de tais transições sob a matriz B deste modelo. A explicação alternativa, mais plausível, é que o músico tocou a nota errada por engano. Conforme mostrado na terceira coluna do gráfico superior direito, esta é a explicação que nosso ouvinte simulado estabelece. No entanto, uma probabilidade diferente de zero é atribuída à possibilidade de que, afinal, fosse a nota certa. A capacidade de relatar esse tipo de incerteza é uma característica fundamental da perspectiva Bayesiana proporcionada pela Inferência Ativa.</p>
<p>O modelo mostrado aqui pode ser mais sofisticado de várias maneiras, mas talvez o mais simples dependa da fatoração do espaço de estados (Mirza et al. 2016). Um exemplo pode ser o tom e a dinâmica da nota (com uma distinção semelhante nos resultados). Em uma tarefa de inferência visual, a fatoração pode ser em quê e onde, o que tem muito valor na neurobiologia (Ungerleider e Haxby 1994). Nas seções subsequentes, apelaremos a esse tipo de fatoração para separar os estados que podem ser influenciados pela criatura em questão daqueles que não podem. Para ler mais sobre esse tipo de modelo (sem ações em jogo) e os tipos de esquema de transmissão de mensagens neuronais que podem ser usados para invertê-lo minimizando a energia livre, veja Parr, Markovic et al. (2019).</p>
<div class="figure">
<img src="images/Figura_7_2.png" alt="" />
<p class="caption">Figura 7.2 Esses gráficos de inferência perceptiva simulados ilustram o processo de atualização de crenças em um exemplo de tentativa baseado no modelo generativo descrito no texto principal. Superior esquerdo: Crenças (probabilidades posteriores) sobre cada nota na sequência em cada passo de tempo. Superior direito: Como os valores numéricos dessas crenças são difíceis de rastrear as crenças no final da sequência, tendo ouvido cada nota (ou seja, crenças retrospectivas) são mostradas. Cada coluna mostra crenças (retrospectivas) sobre os estados ocultos em um determinado intervalo de tempo. Cada linha representa uma hipótese alternativa para esse estado oculto. Quanto mais escuro o sombreamento, mais provável é que a nota tenha sido (com o preto indicando uma probabilidade de um e o branco uma probabilidade de zero). Inferior esquerdo: gradientes de energia livre (negativos) (ou seja, erros de previsão) ao longo do tempo. A taxa de mudança das crenças no gráfico superior esquerdo é determinada pelo valor desses erros em cada passo de tempo. Inferior direito: Sequência de notas musicais apresentadas ao nosso agente sintético (ou seja, as observações que ele recebe durante os passos de tempo 1 a 5). Observe que enquanto no terceiro passo de tempo (o3) o ouvinte ouviu a segunda nota (terceira coluna do gráfico inferior direito), ele infere a terceira nota com maior probabilidade (terceira coluna do gráfico superior direito).</p>
</div>
</div>
<div id="tomada-de-decisão-e-planejamento-como-inferência" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Tomada de decisão e planejamento como inferência<a href="inferência-ativa-em-tempo-discreto-1.html#tomada-de-decisão-e-planejamento-como-inferência" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>O HMM usado acima ilustra uma forma muito simples de inferência categórica baseada em uma sequência de resultados. No entanto, o tipo de criatura (séssil) que isso descreve é bastante desinteressante. Criaturas autônomas são claramente mais do que recipientes passivos de dados sensoriais. Em vez disso, eles mudam ativamente seu ambiente e se envolvem em uma troca bidirecional com seu sensório. Isso fala da importância de converter um HMM em um POMDP, pelo qual devemos inferir não apenas como nosso ambiente está mudando, mas também como nosso curso de ação escolhido o altera e qual curso de ação escolher.</p>
<p>A Figura 7.3 mostra um modelo generativo do POMDP. Isso é o mesmo que foi apresentado no capítulo 4, onde os detalhes da inferência nesse tipo de modelo são descompactados. Observe a semelhança dessa estrutura com o HMM na figura 7.1 e a adição de uma variável extra (π ), na qual estão condicionadas as probabilidades de transição (B). Isso significa que podemos considerar hipóteses alternativas sobre a dinâmica dos estados. Essas hipóteses podem ser interpretadas como planos entre os quais uma criatura pode selecionar. Essa perspectiva equipara avaliação de políticas com comparação de modelos e diz que uma política é simplesmente uma explicação
variável para uma sequência observada de sensações (autogeradas).</p>
<div class="figure">
<img src="images/Figura_7_3.png" alt="" />
<p class="caption">Figura 7.3 POMDP da figura 4.3, descompactando as distribuições de probabilidade em termos de fatores de estado ocultos e modalidades de resultado. (A Figura 7.1 é um caso especial dessa estrutura.) Três pontos a serem observados: Primeiro, a fatoração dos estados ocultos agora significa que a distribuição codificada por A tem (potencialmente) muitos fatores de estado em seu conjunto de condicionamento e não pode mais ser codificada por uma matriz. Em vez disso, isso se torna um objeto tensor, no qual cada índice corresponde a um fator de estado. Em segundo lugar, a separação dos resultados em diferentes modalidades significa que haverá um tensor A separado para cada modalidade. Terceiro, enquanto C e E aparecem no painel à direita, eles não aparecem no gráfico de fatores à esquerda porque só entram no modelo generativo por meio de crenças anteriores sobre políticas. Para uma perspectiva alternativa sobre isso, ver Parr e Friston (2018d) e van de Laar e de Vries (2019).</p>
</div>
<p>O modelo da figura 7.3 difere sutilmente daquele apresentado no capítulo 4: ele permite a fatoração de estados (sobrescrito n) e de resultados (sobrescrito m). A utilidade disso é óbvia quando consideramos a fatoração do mundo visual em onde um objeto está e o que é. Claramente, seria extremamente ineficiente (e incorreria em um alto custo de complexidade) para representar todas as combinações possíveis de localização e identidade, quando a identidade é (normalmente) invariável à localização e vice-versa. Um argumento semelhante pode ser usado para fatoração de tempo de identidade e localização (Friston e Buzsaki 2016). O benefício de introduzir essa fatoração neste estágio é que podemos separar os estados do mundo sobre os quais uma criatura tem controle daqueles que ela não tem. Embora as probabilidades de transição que governam a primeira sejam diferentes em cada política, a segunda será invariável a isso.</p>
<p>Com essas preliminares em vigor, agora esboçamos um exemplo simples de uma tarefa (Friston, FitzGerald et al. 2017) que requer planejamento e ilustra alguns dos principais aspectos da inferência ativa usando POMDPs. Isso envolve um rato em um labirinto em T contendo um estímulo aversivo em um braço, um estímulo atraente em outro e uma pista que indica a localização dos dois estímulos no braço final. Essa configuração significa que o rato pode se comportar de duas maneiras (amplamente). Ele poderia optar por ir direto para um dos dois braços que poderiam conter o estímulo atrativo, arriscando o estímulo aversivo. Alternativamente, ele pode optar por buscar a dica informativa e, em seguida, ir para o braço com maior probabilidade de conter o estímulo atraente.</p>
<p>Essa escolha remete ao clássico dilema prospecção-aproveitamento em psicologia: um dilema que é resolvido sob a Inferência Ativa. A resolução decorre da minimização da energia livre esperada exigida por crenças anteriores sobre políticas. Para revisar isso brevemente (veja o capítulo 4 para detalhes), as políticas mais prováveis (para uma criatura que minimiza sua energia livre variacional) são aquelas que levam à menor energia livre esperada. A energia livre esperada tem a seguinte forma:</p>
<p><span class="math display">\[G(\pi) =
\begin{matrix} \underbrace {\mathbb E_{Q(\tilde s | \pi)}[H[P(\tilde o| \tilde s)]]-H[Q(\tilde o|\pi)]]} \\ Valor\;epistêmico\;negativo\;(−\mathcal I(\pi)) \end{matrix} -
\begin{matrix}\underbrace {\mathbb E_{Q(\tilde s | \pi)}[\ln P(\tilde o | C)] } \\ Valor\;Pragmático  \end{matrix}\qquad (7.4)\]</span>
Essa decomposição da energia livre esperada em valor epistêmico e pragmático destaca o impulso (epistêmico) para a coleta de informações e o impulso (pragmático) para a realização de crenças anteriores (C na figura 7.3).</p>
<p>Tentaremos fornecer uma intuição mais profunda para o valor epistêmico na próxima seção, mas pode ser pensado simplesmente como a quantidade de informação que podemos obter sob uma política específica. A forma do valor pragmático trata efetivamente a probabilidade de resultados, calculada a média de todas as políticas, como se fosse um . Para colocar isso em termos mais intuitivos, se considerarmos um certo tipo. Ao fazê-lo, aquelas políticas com consequências consistentes com este prévio tornam-se mais prováveis, pois estão associadas a menor energia livre de observação esperada para serem muito prováveis, agiremos para cumprir nossa crença de que as encontraremos. Portanto, a probabilidade logarítmica dos resultados pode ser considerada equivalente a uma função de utilidade em outros formalismos, como teoria de controle ótimo e aprendizado por reforço. O fato de que a utilidade e o valor da informação emergem como dois componentes da energia livre esperada significa que não precisamos nos preocupar em equilibrar exploração e exploração. Ambos estão a serviço de otimizar a mesma função.</p>
<div class="figure">
<img src="images/Figura_7_4.png" alt="" />
<p class="caption">Figura 7.4 Probabilidade no contexto 1. Esquerda: configuração do labirinto em T de pistas e estímulos: o estímulo atrativo está à direita e o estímulo aversivo está à esquerda. Direita: Probabilidade ou modelo de observação especifica o mapeamento probabilístico da localização para pistas exteroceptivas (A1) e para pistas interoceptivas (A2). Cada elemento dessas matrizes é a probabilidade do resultado ilustrado no final da linha, condicionado ao contexto ser um e estar no local indicado pela linha. Os resultados exteroceptivos são entradas visuais ou proprioceptivas associadas a cada local, em que a localização do sinal pode dar origem a um sinal para a direita ou para a esquerda. Os resultados interoceptivos são ausentes (círculo com contorno pontilhado), atrativos (círculo preenchido) ou aversivos (círculo não preenchido).</p>
</div>
<p>Para ver como isso se desenrola no exemplo do labirinto em T, precisamos formalizar o modelo generativo da mesma forma que no HMM acima. As Figuras 7.4–7.6 ilustram as probabilidades de verossimilhança e de transição que compõem o modelo generativo para o labirinto em T. Examinaremos isso com alguns detalhes, pois este exemplo mínimo fornece os blocos de construção a partir dos quais os leitores podem construir seus próprios modelos generativos. A primeira coisa a fazer é decidir sobre o número de modalidades de resultados que representam os dados (sensoriais) que nosso modelo deve explicar. Isso nos diz o número de matrizes A que devemos especificar. Aqui, temos duas modalidades que representam dados exteroceptivos referentes a onde o rato está no labirinto (A1) e qual modalidade pode ser os dados interoceptivos que o rato experimenta quando encontra o estímulo atraente (comestível) (A2). Os níveis nessas modalidades (ou seja, as observações alternativas que podem ser feitas em cada uma) determinam as linhas de cada matriz A. A próxima decisão é o número de fatores de estado ocultos que podem ser usados ​​para explicar esses dados; este é o número de matrizes B que precisamos. Consideramos dois fatores aqui: a posição do rato no labirinto e o contexto (estímulo atrativo à esquerda ou à direita). Estes têm quatro e dois níveis, respectivamente. Agora devemos especificar, para cada combinação de estados ocultos, a probabilidade de cada resultado. O contexto 1 é mostrado na figura 7.4; o contexto 2 é mostrado na figura 7.5.</p>
<div class="figure">
<img src="images/Figura_7_5.png" alt="" />
<p class="caption">Figura 7.5 Probabilidade no contexto 2. Quase idêntica à figura 7.4 — neste contexto, os estímulos aversivos e atrativos foram trocados. Isso se reflete na probabilidade dos resultados exteroceptivos na localização da pista e nas probabilidades dos resultados interoceptivos nos braços direito e esquerdo do labirinto.</p>
</div>
<p>Para a primeira modalidade, nosso A1 associa cada local a um resultado com probabilidade um. A localização do “cue”(?) pode estar associada a um “cue”(?) esquerdo ou direito, dependendo do contexto. A modalidade interoceptiva (A2) associa um resultado neutro com os locais de início e “sugestão”(?) e uma chance de 98 por cento de encontrar o resultado atraente quando o contexto corresponde ao braço do labirinto em que o rato entrou. Tecnicamente, essas matrizes A são quantidades de tensores, porque seus elementos são especificados por três números (resultado, localização e contexto), enquanto uma matriz é especificada apenas por dois (linha e coluna).</p>
<p>Em seguida, precisamos especificar as probabilidades de transição. As matrizes B especificam a probabilidade de transição de um estado (coluna) para outro estado (linha), dependendo da escolha da política (π ). Estes especificam as transições referentes à posição do rato no labirinto (B1) e as transições no contexto (B2). A Figura 7.6 mostra as transições B1 controláveis. Cada matriz mostra as probabilidades sob uma escolha de ação diferente (subscrito). Estes permitem um movimento de qualquer local para qualquer outro local, exceto dos dois braços do labirinto, que são estados absorventes. Isso significa que, uma vez lá, o rato deve permanecer lá, independentemente das ações que escolher. Em contraste, o rato não tem controle sobre o contexto (ou seja, se está no contexto 1, mostrado na figura 7.4, ou no contexto 2, mostrado na figura 7.5). O contexto permanece constante ao longo do tempo e pode ser representado como uma matriz de identidade:</p>
<p><span class="math display">\[ B^2_\pi = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \qquad (7.5)\]</span>
Aqui cada coluna (e linha) refere-se a um estado indexando a figura 7.4 ou a figura 7.5. Isso significa que qualquer contexto em que começamos permanece constante (transições para si mesmo ) ao longo do tempo. Isso é verdade independentemente da política selecionada. O vetor C1 mostra preferências anteriores para cada um dos resultados nesta modalidade, com preferências uniformes, exceto por uma leve aversão (−1) ao local de início.</p>
<div class="figure">
<img src="images/Figura_7_6.png" alt="" />
<p class="caption"><strong>Figura 7.6</strong> Probabilidades de transição controláveis para movimentação entre os diferentes locais. Cada uma das quatro matrizes corresponde a uma ação alternativa que o rato pode escolher. Estes permitem uma mudança de qualquer estado (exceto o braço direito e esquerdo) para qualquer outro estado. Os braços direito e esquerdo são estados absorventes, nos quais o rato deve permanecer uma vez inserido.</p>
</div>
<p>O vetor C2 especifica preferências (+6) para o estímulo atrativo e aversão (-6) para o estímulo aversivo. A ausência de qualquer um é considerada neutra (0).</p>
<p><span class="math display">\[ \begin{equation}
C^1 = \sigma([-1,0,0,0,0]^T) \qquad\qquad\qquad\\
C^2 = \sigma([0,6,-6]^T)
\end{equation} \qquad\qquad\qquad (7.6) \]</span>
A ordem dos elementos nesses vetores corresponde à ordem das linhas nas matrizes A correspondentes. A função softmax (σ ) permite especificar preferências em termos de valores positivos e negativos (correspondentes a probabilidades logarítmicas não normalizadas), que são então convertidas em probabilidades. Isso preserva a diferença nas probabilidades de log (ou a probabilidade relativa) enquanto garante a normalização. Praticamente, esta formulação significa que o estímulo atrativo é considerado e6 (≈ 400) vezes mais provável que o estímulo neutro no modelo generativo do rato. Esta é uma preferência muito forte que significa que o rato acredita que suas ações são muito mais propensas a levar ao resultado atraente. Essa restrição à inferência sobre a ação é crucial para o comportamento que se segue. Finalmente, os vetores D especificam as probabilidades anteriores para os estados iniciais:</p>
<p><span class="math display">\[ \begin{equation}
D^1 = [1,0,0,0,0]^T \qquad\qquad\qquad\\
D^2 =\frac{1}{2}[1,1]^T
\end{equation} \qquad\qquad\qquad (7.7) \]</span></p>
<p>A ordem dos elementos nestes vetores coincide com a das matrizes B. O vetor D1 indica uma crença confiante em começar no centro do labirinto. O vetor D2 indica que os dois contextos (figura 7.4 ou 7.5) são considerados igualmente prováveis no início.</p>
<p>A Figura 7.7 mostra o que acontece quando invertemos o modelo generativo das figuras 7.4–7.6. A linha superior ilustra o que veríamos se observássemos o comportamento do rato. Começa no centro e depois vai para a dica informativa. Isso se deve ao alto valor epistêmico associado a esse local (ou seja, as observações feitas nesse local têm o potencial de resolver a incerteza sobre o contexto). Ao ver a pista que indica um contexto à esquerda (contexto 1), o rato escolhe o braço esquerdo do labirinto e encontra o estímulo recompensador. Este movimento é impulsionado pelo alto valor pragmático atribuído a este local. Os gráficos inferiores ilustram a atualização de crenças que ocorre durante este teste simples. Como na figura 7.2, isso é mostrado na forma que poderíamos esperar observar em um rato idealizado se estivéssemos medindo a atividade neuronal (ou seja, taxas de disparo e potenciais de campo locais [LFPs]). Observe a rápida mudança nas crenças no segundo passo de tempo, quando o rato atinge o local da dica informativa e a LFP associada (linha tracejada).</p>
<div class="figure">
<img src="images/Figura_7_7.png" alt="" />
<p class="caption"><strong>Figura 7.7</strong> Comportamento epistêmico e pragmático simulado de um rato forrageando em um labirinto em T. O rato começa no local central, mas depois escolhe amostrar a sugestão informativa no braço inferior do labirinto. Esta localização está associada ao maior valor epistêmico, pois observar a deixa neste local revela o contexto (recompensa direita ou esquerda) em que o rato se encontra. LFP (ε ). Sem mais incertezas para resolver, o rato seleciona a opção pragmaticamente valiosa e vai para o braço esquerdo do labirinto. Os dois gráficos à direita mostram as crenças mantidas pelo rato no final da tentativa sobre todos os tempos anteriores (ou seja, são crenças retrospectivas e não as crenças do rato no momento da decisão). Ele acredita (corretamente) que começou no local central, foi para o braço do taco e depois foi para o braço esquerdo. Para o fator de estado oculto do contexto, o rato acredita que o contexto foi o contexto da esquerda por toda parte.</p>
</div>
</div>
<div id="busca-de-informações" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Busca de informações<a href="inferência-ativa-em-tempo-discreto-1.html#busca-de-informações" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A simulação na seção 7.2 ilustra um exemplo simples de um trade-off prospecção-aproveitamento, que é resolvido pela busca de informações até que a incerteza seja resolvida, então explorando o que foi inferido para atender às preferências anteriores. Nesta seção, descompactamos o conceito de valor epistêmico com mais detalhes. Como vimos na equação 7.4, isso compreende dois termos:</p>
<p>$$
<span class="math display">\[\begin{equation}
\begin{matrix} \underbrace{\mathcal I(\pi)} \\ valor\;epistêmico\end{matrix}
= \begin{matrix} \underbrace{H(Q(\tilde o|\pi))} \\ entropia\;preditiva\;posterior \end{matrix}
- \begin{matrix} \underbrace{E_{Q(\tilde s|\pi)}[H[P(\tilde o| \tilde s)]]} \\ ambiguidade\;esperada\end{matrix} \\
= \begin{matrix} \underbrace{D_{KL}[P(\tilde o | \tilde s)Q(\tilde o | \tilde \pi) || Q(\tilde o | \tilde \pi) Q(\tilde s | \tilde \pi)]} \\ informação\;mútua \end{matrix} \qquad\qquad\qquad (7.8) \\
= \begin{matrix} \underbrace{E_{Q(\tilde o | \tilde \pi)}[D_{KL}][Q(\tilde s |\pi, \tilde o) || Q(\tilde s | \pi)]} ; Q(\tilde s|\pi, \tilde o) \triangleq \frac{P(\tilde o| \tilde s)Q(\tilde s | \pi)}{Q(\tilde o | \pi)} \\ ganho\;de\;informação,\;saliência,\;surpresa\;Bayesiana \end{matrix}  \\

\end{equation}\]</span>
$$
Estas são a entropia preditiva posterior e a ambiguidade esperada, respectivamente. Abaixo destes, destacamos a correspondência entre estes e outros rearranjos. Para descompactá-los de forma intuitiva, vamos enquadrar isso em termos de um paradigma visual, onde sacadas alternativas (π ) levam a diferentes transições entre locais de fixação (s). Além dos locais de fixação, os estados ocultos incluem a identidade de um estímulo em cada local. Uma combinação de estímulo e fixação gera consequências visuais e proprioceptivas (o). Com isso em mente, podemos interpretar a entropia preditiva posterior como a dispersão (ou incerteza) associada ao “o que eu veria se realizasse esse movimento ocular”. Do ponto de vista de um cientista, isso quantifica o quão incertos podemos estar sobre os dados que obteríamos ao realizar um determinado experimento. Sob essa perspectiva, faz sentido selecionarmos as sacadas (ou experimentos) que estão associadas à maior entropia preditiva posterior, pois oferecem o maior potencial de resolução de incertezas. Não ganharíamos nada realizando um experimento se já soubéssemos quais seriam os resultados com um alto grau de confiança.</p>
<p>No entanto, a entropia preditiva apenas nos diz a quantidade total de incerteza. Não nos diz quanta incerteza é realmente solucionável. Sempre estaremos incertos sobre o próximo número em uma sequência de números gerados aleatoriamente, mas nunca resolveremos nossa incerteza sobre o processo que os gerou fixando neles. É aí que entra a ambiguidade esperada. Isso quantifica o grau em que as observações e os estados são independentes um do outro. Se os estados sempre gerarem a mesma observação, essa quantidade será zero. Será máximo se, como no gerador de números aleatórios, não houver associação entre estados e resultados. No domínio visual, isso implica que a melhor sacada será aquela em direção a um estímulo bem iluminado, onde há pouca ambiguidade sobre “o que eu veria se olhasse para esse estímulo”. Tomados em conjunto, isso diz que as melhores sacadas (ou seja, experimentos perceptivos) são aquelas para as quais há a maior incerteza para resolver (entropia preditiva posterior), mas somente se essa incerteza puder ser resolvida (ambiguidade negativa). Curiosamente, isso tem exatamente a mesma forma que as expressões desenvolvidas em estatística para pontuar o projeto experimental em termos de ganho de informação (Lindley 1956).</p>
<p>A Figura 7.9 ilustra o que acontece em um paradigma sacádico (Parr e Friston 2017b) quando simulamos manipulações para a ambiguidade e entropia preditiva posterior. Isso mostra quatro estímulos (quadrados), cada um dos quais pode mudar de cor a cada momento. Sobreposto a estes está um traço de rastreamento ocular simulado, como se estivéssemos medindo para onde um participante experimental estava olhando. Crucialmente, especificamos crenças anteriores sobre resultados como uniformes (ou seja, valor pragmático a ser ausente), impedindo quaisquer escolhas baseadas em preferências. Isso significa que cada sacada é selecionada para maximizar o valor epistêmico. Quando o modelo generativo trata todos os quatro estímulos como equivalentes (imagem da esquerda), todos são amostrados com aproximadamente a mesma frequência. No entanto, podemos modular a incerteza associada a cada estímulo (ver caixa 7.1). Se definirmos um estímulo para ter uma ambiguidade maior (aumentando o valor dos elementos fora da diagonal da matriz A correspondente), esse quadrado será ignorado (imagem do meio). Este é um exemplo do famoso efeito “streetlight” (Demirdjian et al. 2005), que leva o nome da metáfora de pessoas que perderam suas chaves tarde da noite. O primeiro lugar que eles podem procurar é sob a luz da rua - não porque as chaves provavelmente estejam lá, mas porque é o melhor lugar para encontrar informações de alta qualidade, inequívocas e que resolvem incertezas. A simulação mostra como a praça ambígua (por exemplo, mal iluminada) é ignorada, reproduzindo um efeito “streetlight” in silico.</p>
<p>Em contraste, a imagem da direita na Figura 7.8 mostra o que acontece quando tornamos as transições menos previsíveis para o quadrado inferior esquerdo. Acumulamos incerteza sobre esta localização muito rapidamente, garantindo uma alta entropia preditiva posterior sem alteração da ambiguidade. Como podemos ver, isso leva a uma fixação mais frequente nesse local, pois sempre há novas incertezas a serem resolvidas aqui. Intuitivamente, se eu sei que algo tem uma dinâmica muito previsível, não preciso olhar para ele com muita frequência para ter certeza de seu estado. Por outro lado, se algo pode ter mudado no tempo em que estive olhando para outra coisa, vale a pena olhar para trás para verificar. Essas simulações são projetadas para oferecer uma intuição para as duas partes do valor epistêmico, para ver como a minimização da energia livre esperada garante que selecionemos ativamente nossos dados sensoriais para descobrir o mundo.</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th>Quadro 7.1 Incerteza e precisão</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>O exemplo da Figura 7.7 apela ao conceito de precisão – uma ideia importante neste livro. A precisão é o inverso da variância e pontua nossa confiança em uma determinada distribuição de probabilidade. Isso está intimamente relacionado à entropia negativa (negentropia) de uma distribuição:<span class="math display">\[ -H[P(s)]=\mathbb E_{P(x)}[\ln P(s)]\]</span>Uma maneira simples de parametrizar uma distribuição de forma que ela possa ser mais ou menos precisa é usar uma forma de Gibbs com um parâmetro de temperatura inverso (<span class="math inline">\(omega\)</span>) tem a seguinte forma: <span class="math display">\[ P(s |\omega )=Cat(\sigma(\omega\ln D))\]</span>Observe que a precisão multiplica o log anterior, para que possa ser interpretado como um dispositivo de controle de ganho (amplificador em vez de adicionar sinais neurais). Os gráficos na figura 7.8 mostram como a distribuição de probabilidade (cada coluna representando a probabilidade de um estado alternativo) muda para um dado D quando variamos ω. Observe o aumento da confiança com o aumento da precisão.<img src="images/Figura_7_8.png" alt="Figura 7.8" />Este tipo de parametrização pode ser aplicado a qualquer uma das distribuições utilizadas em um POMDP. Além disso, podemos definir a priori sobre a precisão e inferir isso da mesma forma que inferimos outras variáveis latentes (ou seja, através da minimização da energia livre). Supondo que o prior tenha uma distribuição Gamma (excluindo valores negativos da precisão), obtemos as seguintes atualizações (consulte o apêndice B para obter detalhes):<span class="math display">\[P(\omega)=\ \Gamma(1,\pmb{\beta}_\omega)\]</span><span class="math display">\[Q(\omega)=\Gamma(1,\beta_\omega)\]</span><span class="math display">\[\Longrightarrow\pmb{\dot\beta_\omega}=(\pmb{D}^{\beta_\omega^{-1}}-s)\cdot\ln \pmb{D}+\beta_w-{\pmb{\beta}_w}\]</span>Há um crescente reconhecimento de que o substrato biológico desses parâmetros de precisão podem ser os sistemas neuromoduladores que determinam o ganho das respostas neurais. O Capítulo 5 discute as evidências que relacionam esses parâmetros a neuroquímicos específicos.</td>
</tr>
</tbody>
</table>
<div class="figure">
<img src="images/Figura_7_9.png" alt="" />
<p class="caption">Figura 7.9 Paradigma de busca visual epistêmica simulada (Parr e Friston 2017b) com o traço sintético de rastreamento ocular sobreposto aos quatro locais de estímulo. Cada estímulo (quadrado sombreado) está associado a uma matriz de transição que pode ser mais ou menos previsível e uma matriz de probabilidade que pode ser mais ou menos ambígua. Esquerda: Quando as transições e probabilidades são igualmente previsíveis para todas as quatro localizações, todas as localizações são amostradas com aproximadamente a mesma frequência. Meio: O visualizador mostra aversão ao quadrado superior esquerdo quando é especificado com um mapeamento de probabilidade menos preciso (mais ambíguo). Direita: O quadrado inferior esquerdo é epistemicamente atraente quando as probabilidades de transição são especificadas como mais incertas.</p>
</div>
</div>
<div id="aprendizado-e-novidade" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Aprendizado e Novidade<a href="inferência-ativa-em-tempo-discreto-1.html#aprendizado-e-novidade" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As seções 7.2–7.4 estabelecem tudo o que é necessário para a maioria das aplicações práticas da Inferência Ativa. No entanto, assumimos que o modelo generativo já é conhecido e não muda como efeito da aprendizagem. Em algumas aplicações práticas, podemos querer considerar como uma ou mais partes do modelo generativo (por exemplo, a matriz A ou B) são aprendidas durante um experimento ou, mais amplamente, como otimizamos a estrutura do próprio modelo generativo , dados alguns dados (Friston, FitzGerald et al. 2016). Ao fazer isso, a Inferência Ativa se estende ao aprendizado ativo, e a saliência (equação 7.5) que descreve o ganho de informação sobre os estados é complementada pela novidade, que trata da resolução da incerteza sobre (por exemplo) os elementos da matriz A mostrada na equação 7.1, a matriz B mostrada na equação 7.2, ou quaisquer outros parâmetros do modelo generativo. Essas crenças agora podem variar com o tempo, em vez de serem fixas, como se supunha até agora (Schwartenbeck et al. 2019). Para chegar a isso, primeiro temos que estender o modelo generativo como na Figura 7.10 para incluir crenças sobre esses parâmetros do modelo.</p>
<div class="figure">
<img src="images/Figura_7_10.png" alt="" />
<p class="caption"><strong>Figura 7.10</strong> Esse modelo generativo para aprendizado usa a mesma estrutura POMDP da figura 7.3, mas as prioridades para cada um dos estados ocultos agora dependem de variáveis ​​(em círculos), que agora vêm equipadas com crenças anteriores. Estas têm a forma de distribuições de Dirichlet, que são conjugadas (ver quadro 7.2) às distribuições categóricas consideradas até agora. O modelo mostra como a probabilidade de resultados dados estados agora também depende de uma variável A (que é a mesma para todos os pontos de tempo), as probabilidades de transição agora estão condicionadas a uma variável B, as preferências dependem de C, os estados iniciais dependem em D, e a política de forma fixa a priori depende de E. Ao tornar explícitas as crenças prévias sobre os parâmetros do modelo generativo, essa figura enfatiza que tanto a inferência quanto o aprendizado são processos de minimização de energia livre, mas são distintos. Em suma, a inferência descreve a otimização de crenças sobre o estado do mundo como ele é (s), incluindo crenças sobre a maneira como estamos agindo (π ). Em contraste, a aprendizagem descreve a otimização de crenças sobre as relações entre essas variáveis ​​(A,  B,  C,  D ou E ). As últimas variam muito mais lentamente que as primeiras e só podem ser aprendidas quando os estados foram inferidos. Voltaremos a essa separação de escalas de tempo abaixo quando considerarmos os modelos generativos hierárquicos.</p>
</div>
<p>Conceitualmente, incluir crenças sobre parâmetros no modelo generativo permite tratar a aprendizagem como outra forma de inferência Bayesiana – ou seja, como a passagem de crenças anteriores para posteriores sobre parâmetros do modelo. Isso destaca a semelhança fundamental entre percepção e aprendizado: da mesma forma que a percepção pode ser descrita como a inversão de um modelo generativo para inferir estados ocultos a partir de observações, o aprendizado pode ser descrito como a inversão de um modelo generativo para incluir crenças sobre parâmetros ( embora normalmente esta inversão possa operar em uma escala de tempo mais lenta).</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th>Quadro 7.2 Antecedentes do Conjugado</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ao configurar um modelo generativo da forma da figura 7.10, é importante selecionar cuidadosamente a distribuição apropriada para crenças anteriores. Normalmente, esta será a distribuição anterior conjugada associada à probabilidade. Uma crença prévia conjugada significa que, quando usada para realizar inferência Bayesiana, a crença posterior será do mesmo tipo de distribuição. Por exemplo, usando a regra de Bayes:<span class="math inline">\(P(D|s)\propto P(D) P(s|D)\)</span> Se <span class="math inline">\(P(s|D)\)</span> é uma distribuição categórica, quando escolhemos uma distribuição de Dirichlet (conjugada para categórica ) para <span class="math inline">\(P(D)\)</span>, podemos garantir que <span class="math inline">\(P(D|s)\)</span> também é uma distribuição de Dirichlet. Colocando formalmente:<span class="math inline">\(\begin{matrix}P(D)=Dir(d) \\ P(s|D)=Cat(D)\end{matrix}\bigg \}\Rightarrow P(D|s) = Dir(d)\)</span></td>
</tr>
</tbody>
</table>
<p>A maneira mais simples de escolher o tipo certo de a priori é procurar a priori conjugada para qualquer forma que a distribuição de verossimilhança assuma. Para as distribuições categóricas usadas aqui, uma distribuição de Dirichlet é a escolha apropriada para crenças sobre parâmetros (ver caixa 7.2). Tendo incluído essas crenças anteriores adicionais, podemos agora otimizar as crenças posteriores sobre a estrutura do modelo generativo. Isso significa incorporá-los à energia livre (como fizemos para os estados no capítulo 4) e encontrar os mínimos de energia livre.</p>
<p><span class="math display">\[\begin{equation}
\theta = (A, B, C,D,E) \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\\
F=\mathbb E_{Q(\pi,\theta)}[F(\pi,\theta)]+D_{KL}[Q(\theta)||P(\theta)]+D_{KL}[Q(\pi)||P(\pi)]
\end{equation} \qquad (7.9)\]</span>
As distribuições de Dirichlet são parametrizadas por contagens (ou pseudocontagens) que indexam o número de vezes que uma determinada variável categórica foi vista (ou, no caso das anteriores, como se tivesse sido vista esse número de vezes). Para a derivação das regras de atualização para esses parâmetros, consulte o apêndice B. Por enquanto, resumimos a regra de atualização e as principais propriedades de uma distribuição de Dirichlet, focando nos parâmetros a e a de concentração associados ao anterior e posterior sobre A.</p>
<p><span class="math display">\[ \begin{equation} \qquad
\pmb{a} = a + \sum_{\tau}\pmb{s}_\tau \otimes\theta_\tau \\
\mathbb E_Q[A_{ij}]=\pmb{A}_{ij} \approx \frac{\pmb{a}_{ij}}{\pmb{a}_{0,j}} \qquad\\
\mathbb E_Q[\ln A_{ij}]= \ln \pmb{A}_{ij}=\psi(\pmb{a}_{ij})-psi(\pmb{a}_{oj})  \qquad (7.10) \\
\pmb{a}_{oj} \triangleq \sum_k \pmb{a}_{kj}
\end{equation}
\]</span></p>
<p>A primeira linha aqui expressa a atualização dos parâmetros de concentração anteriores para posteriores seguindo uma série de observações, com crenças sobre os estados que os causaram. A cruz no círculo indica um produto tensorial de Kronecker (ou produto externo no caso de dois vetores), aqui dando origem a uma matriz em que cada elemento é o produto de um par de elementos em <span class="math inline">\(o_\tau\)</span> e <span class="math inline">\(\pmb{s}_\tau\)</span> . Esta regra de atualização pode ser interpretada simplesmente como uma forma de plasticidade dependente da atividade. Quando um resultado é observado em combinação com uma crença posterior de que um determinado estado o causou, o elemento da matriz que representa a relação entre os dois é incrementado.</p>
<p>A segunda linha da equação destaca a interpretação dos parâmetros de concentração de Dirichlet em termos de contagens. Para um determinado estado (coluna), cada elemento de a é o número de vezes que o resultado correspondente foi visto. Dividindo pela soma dos elementos na coluna (número total de observações ou pseudo-observações) dá a probabilidade de cada resultado dado aquele estado. Para entender por que esse (pseudo) método de contagem faz sentido intuitivo, considere o exemplo do músico amador do início deste capítulo. Se contarmos quantas vezes o músico toca a primeira nota quando pretende fazê-lo (primeira linha e coluna), quantas vezes ele toca a segunda nota quando pretende fazê-lo (segunda linha e coluna), e assim por diante, e dividi-los pelo número total de vezes que ela pretende cada nota, um acabará por convergir para os valores numéricos corretos da matriz A mostrada na equação 7.1 - ou seja, que o músico atinge todas as notas pretendidas 70% das vezes. O método de contagem tem outra consequência importante à qual retornaremos: o número de contagens ou pseudocontagens que precedem uma observação nos diz a probabilidade de atualizarmos nossas crenças ao fazer a observação. Imagine jogar uma moeda cinco vezes e obter cinco caras seguidas. Isso pode nos levar a atualizar nossas crenças para favorecer a hipótese de que esta é uma moeda injusta. No entanto, se isso tivesse sido precedido por 100 lançamentos com 50 caras e 50 coroas, as cinco caras finais fariam pouco para influenciar nossas crenças sobre se esta é uma moeda justa. A terceira linha da equação 7.10 mostra uma identidade útil associada às distribuições de Dirichlet: o logaritmo esperado da variável aleatória é dado pela diferença em duas funções digamma (derivada de uma função gama).</p>
<p>A abordagem inferencial ao aprendizado destaca uma diferença importante entre a Inferência Ativa e a maioria das outras abordagens de neurociência computacional e aprendizado de máquina, que incorporam várias regras de aprendizado (por exemplo, regras Hebbianas ou retropropagação de erros) que são consideradas biologicamente realista ou computacionalmente eficiente. Na Inferência Ativa, as regras de atualização que governam o aprendizado são derivadas de considerações estatísticas, mas acabam sendo notavelmente semelhantes às regras biologicamente motivadas para plasticidade dependente de atividade (veja as considerações acima na primeira linha da equação 7.10). Isso exemplifica um dos apelos das abordagens normativas, que partem dos primeiros princípios para explicar o que sabemos sobre cérebros e comportamento – e coisas que não sabíamos.</p>
<p>Outra diferença entre a Inferência Ativa e a maioria das abordagens de aprendizado de máquina é que o aprendizado é naturalmente descrito como um processo ativo, no qual as criaturas selecionam de forma autônoma os dados mais apropriados para melhorar seus modelos generativos. Isso fica evidente se considerarmos que ao incluir crenças sobre parâmetros no modelo, a energia livre esperada adquire um termo adicional:</p>
<p><span class="math display">\[
\begin{equation}
G(\pi)= \begin{matrix}\underbrace{D_{KL}[Q(\tilde o|\pi)||P(\tilde o|C)]} \\ Risco \end{matrix}+
\begin{matrix}\underbrace{\mathbb E_{Q(\tilde s|\pi)}[H[P(\tilde o|\tilde s)]] } \\ Ambiguidade \end{matrix} \\
\begin{matrix}\underbrace{+ \mathbb E_{\tilde Q(\tilde o,\tilde s, \tilde \theta | \pi )}[\ln Q(\theta) - \ln P(\theta)|\tilde o, \tilde s) ] } \\ Parâmetro \;de\; Ganho\;de\;informação \end{matrix} \\
\begin{matrix}\underbrace{= - \mathbb E_{Q(\tilde o, \tilde s| \pi)}[D_{KL}[Q(\tilde s|\pi,\tilde o)||Q(\tilde s| \pi )]] } \\ Saliência \end{matrix} \\
\begin{matrix}\underbrace{- \mathbb E_{Q(\tilde o,\tilde  s|\pi)}[D_{KL}[Q(\theta|\tilde o,\tilde s)|| Q(\theta )]] } \\ novidade \end{matrix} -
\begin{matrix}\underbrace{\mathbb E_{Q(\tilde o |\pi)}[\ln P(\tilde o|C)] } \\ Valor\;pragmático \end{matrix}
\end{equation}
\]</span>
Os termos de saliência e valor pragmático já existiam na equação 7.4, mas o termo novidade é novo. A igualdade final aqui mostra um arranjo que destaca a relação entre saliência e novidade. Em suma, a saliência é inferir o que a novidade é para a aprendizagem. Ambos são expressões da mudança nas crenças antecipadas uma vez que um experimento perceptual (ou seja, uma ação em uma política) é realizado. Tal como acontece com os experimentos científicos, quanto maior a mudança nas crenças após a coleta de dados, melhor o experimento. Voltando à analogia de jogar uma moeda e acumular contagens, isso nos diz algo útil. Se tivermos duas moedas e pudermos jogar qualquer uma delas, podemos provocar a maior mudança nas crenças jogando a moeda que havíamos lançado apenas cinco vezes antes, em vez da moeda com 100 jogadas anteriores. Há uma maior novidade associada ao lançamento da moeda anterior (menos familiar). Da mesma forma, se tivermos crenças anteriores confiantes como se tivéssemos observado algo muitas vezes, as políticas que questionam essas variáveis ​​estão associadas a menos novidade do que aquelas sobre as quais temos crenças menos confiantes.</p>
<p>Para ilustrar como isso funciona na prática, imagine que temos uma criatura muito míope de pé sobre um piso de ladrilhos. Esta criatura só pode ver a cor do ladrilho em que está e só pode mover um ladrilho de cada vez. Para qualquer paisagem adequadamente grande com muitos ladrilhos, é computacionalmente muito caro representar a cor de cada ladrilho como um estado oculto diferente. No entanto, uma forma mais simples de modelo está disponível. Se associarmos estados ocultos apenas com localização e cores apenas com resultados, podemos representar com eficiência crenças sobre “o que eu veria se fosse até lá” na matriz A que gera ladrilhos coloridos a partir de locais. Ao acumular parâmetros de Dirichlet (equação 7.10), nossa criatura pode otimizar essas crenças com base em observações. Podemos interpretar isso como uma forma de memória sináptica em oposição à manutenção da atividade persistente em neurônios que representam crenças sobre a cor de um determinado azulejo. Dado esse tipo de modelo generativo, em que toda a incerteza está nos parâmetros da distribuição de verossimilhança, é interessante ver o que acontece na ausência de quaisquer preferências (ou seja, quando o termo novidade da equação 7.11 domina a seleção de políticas). A Figura 7.11 mostra uma simulação de um ambiente simples composto por 64 ladrilhos pretos ou brancos. À medida que cada ladrilho é visitado, as crenças sobre a probabilidade de observar preto ou branco naquele local são atualizadas através do acúmulo de parâmetros de Dirichlet. Como grandes parâmetros de Dirichlet impedem grandes atualizações de crenças, o impulso para a resolução de novidade dada pela minimização de energia livre esperada leva nossa criatura simulada a evitar quaisquer locais visitados anteriormente.</p>
<p>Os mesmos princípios podem ser aplicados a uma série de outros paradigmas (por exemplo, se reinterpretarmos o caminho percorrido por nossa criatura como um caminho de varredura sacádica, isso pode ser aplicado à amostragem visual ativa). No domínio da visão ativa, isso foi usado para simular os tipos de comportamento de busca visual induzidos por tarefas de cancelamento de alvo (Parr e Friston 2017a). Posteriormente, foram demonstradas evidências da plasticidade de curto prazo necessária para acumular parâmetros de Dirichlet nesse cenário (Parr, Mirza et al. 2019).</p>
<p>Assim como podemos estender ideias sobre inferência para aprendizado, é possível ir (pelo menos) um passo adiante e pensar em aprendizado de estrutura: o processo de não apenas otimizar os parâmetros no modelo, mas selecionar entre diferentes modelos com mais ou menos parâmetros em jogo. O Quadro 7.3 apresenta uma maneira de fazer isso que envolve comparações post hoc eficientes de modelos hipotéticos alternativos. Isso tem sido usado como uma metáfora para o sono (Friston, Lin et al. 2017) e atividade espontânea em repouso (Pezzulo, Zorzi e Corbetta 2020), onde não são coletados novos dados, mas a estrutura do modelo ainda pode ser refinada e simplificada .</p>
<div class="figure">
<img src="images/Figura_7_11.png" alt="" />
<p class="caption"><strong>Figura 7.11</strong> A aprendizagem ativa é demonstrada por uma criatura sintética explorando um mundo simples de azulejos pretos e brancos (Bruineberg et al. 2018, Kaplan e Friston 2018). Esquerda: Caminho percorrido pela criatura, mostrando quais peças são brancas e quais são pretas (os pontos correspondem aos locais visitados). Direita: Uma matriz da criatura e as crenças (em termos de contagens de Dirichlet normalizadas) que a criatura tem sobre o que veria indo para locais diferentes. As células na matriz A são brancas (ou pretas) se a criatura acredita fortemente que a peça correspondente é branca (ou preta); eles são cinzas se a criatura não tiver certeza sobre a cor. Crucialmente, essas crenças influenciam o caminho que ele toma por meio do termo novidade da energia livre esperada. Aqueles locais sobre os quais ele tem crenças confiantes oferecem relativamente poucas oportunidades para a resolução de incertezas, então ele não os revisita. Em outras palavras, o fenômeno da “inibição do retorno” (Posner et al. 1985) surge naturalmente da minimização da energia livre esperada.</p>
</div>
</div>
<div id="inferência-hierárquica-ou-profunda" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Inferência Hierárquica ou Profunda<a href="inferência-ativa-em-tempo-discreto-1.html#inferência-hierárquica-ou-profunda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Na seção anterior, vimos um método para extensão hierárquica do modelo generativo original baseado na definição de priors sobre os parâmetros do modelo generativo. A Figura 7.12 mostra uma segunda forma de hierarquia que se refere ao aninhamento de escalas temporais. Este modelo generativo para inferência hierárquica ou profunda pode ser concebido como uma extensão hierárquica do modelo raso mostrado na figura 7.3: inclui uma série de modelos POMDP no nível inferior que são os mesmos da Figura 7.3 (um exemplo é delineado com a caixa tracejada), contextualizada por um POMDP de nível superior.</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Quadro 7.3</strong> Aprendizado de estrutura e redução de modelo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A discussão na seção 7.4 trata de uma forma importante, mas limitada, de aprendizado (paramétrico). O próximo nível de sofisticação – aprender sobre a estrutura do mundo – vai além da otimização dos parâmetros do modelo e pergunta se devemos expandir ou podar a estrutura do modelo. Isso pode ser colocado como uma questão de comparação de modelos (Friston, Lin et al. 2017). Em outras palavras, minha energia livre aumentaria ou diminuiria se eu (por exemplo) eliminasse elementos de uma matriz de probabilidade? Ao comparar modelos com e sem esses elementos, podemos responder a essa pergunta. No entanto, pode ser muito caro ter que inverter explicitamente vários modelos. Felizmente, um método eficiente para fazer isso - conhecido como redução do modelo bayesiano (Friston, Litvak et al. 2016; Friston, Parr e Zeidman 2018) - está disponível e requer apenas a inversão de um único modelo completo. Em um cenário geral, a comparação entre um modelo completo e um com anteriores alternativos (indicado por ~) pode ser alcançada através das seguintes fórmulas:<span class="math display">\[\begin{equation} \Delta F = F[P(\theta)]-F[P(\theta)]=\ln \mathbb E_{Q(\theta)}\Big [\frac{P(\theta)}{\tilde P(\theta)}\Big] \\ \tilde Q(\theta) \propto exp(\ln Q(\theta)+\ln \tilde P(\theta)) - \ln P(\theta)) +\Delta F) \end{equation}\]</span>Para os priors de Dirichlet usados na seção 7.5, isso assume a forma (onde B é a função beta multivariada):<span class="math display">\[\begin{equation} \Delta F = \ln B(\tilde d)- \ln B(d) + \ln B(\pmb d)-\ln B(\tilde {\pmb d}) \\ \pmb {\tilde d}=\pmb d+\tilde d-d \end{equation}\]</span>Essa forma de redução do modelo pode ser importante para entender a otimização do modelo offline, do tipo que pode ocorrer durante o sono. Revisitaremos brevemente a redução do modelo bayesiano no capítulo 8, ao considerar a otimização de modelos hierárquicos com componentes discretos e contínuos.</td>
</tr>
</tbody>
</table>
<p>É importante ressaltar que esse modelo generativo inclui variáveis que evoluem em diferentes escalas de tempo: mais lento para níveis mais altos e mais rápido para níveis mais baixos (Friston 2008; Friston, Rosch et al. 2017; Pezzulo, Rigoli e Friston 2018). Isso fica evidente se considerarmos que os modelos POMDP no nível 1 evoluem ao longo de três etapas de tempo, mas cada uma dessas curtas trajetórias de estados e resultados depende de um único estado no nível superior (nível 2) que persiste ao longo de toda a trajetória no nível nível mais baixo. Em outras palavras, para cada passo de tempo da perspectiva do nível superior, existem vários (aqui, três) passos de tempo para o nível inferior.</p>
<p>Para obter alguma intuição para essa separação de escalas de tempo – que subscreve uma profunda inferência temporal – vale a pena pensar em um exemplo simples de hierarquia na vida cotidiana: a leitura. Fazemos inferências sobre palavras que se combinam para formar frases. As frases se combinam para formar parágrafos, páginas, livros, bibliotecas e assim por diante. Se imaginarmos que cada estado no nível inferior da figura 7.12 é uma palavra, cada estado no nível superior pode ser pensado como uma sentença. Crucialmente, a duração da frase transcende a de qualquer palavra na sequência.</p>
<div class="figure">
<img src="images/Figura_7_12.png" alt="" />
<p class="caption"><strong>Figura 7.12</strong> Podemos estender o modelo generativo (superficial) apresentado na Figura 7.3 para que ele permita inferência hierárquica ou profunda, que evolui em várias escalas de tempo. O modelo generativo completo inclui um contexto de mudança lenta (no nível 2) que gera uma série de trajetórias curtas no nível 1 inferior e mais rápido. A forma do POMDP é a mesma no nível superior e no nível inferior (um dos POMDPs é delineado com a caixa tracejada). A única diferença é que ele se estende no tempo (horizontal) e que os resultados que gera não são observados diretamente. Em vez disso, eles formam priores empíricos para o nível inferior, o que gera resultados observáveis.</p>
</div>
<p>O exemplo de leitura é ilustrado com mais detalhes na figura 7.13, que é baseada no exemplo de Friston, Rosch et al. (2017), ao qual nos referimos para mais detalhes. O modelo está estruturado como na figura 7.12 e representa frases (no nível superior) e palavras (no nível inferior) extraídas de uma linguagem muito simples. Esta linguagem compreende três palavras possíveis ( fugir, alimentar, esperar) que podem ser organizadas em seis possíveis frases de quatro palavras. Se a frase for “fuja, espere, alimente, espere”, o nível mais alto prevê a palavra fugir para o primeiro dos POMDPs de nível inferior, aguarde o segundo e assim por diante. No nível inferior, começamos com uma prévia empírica (D) baseada no nível superior, que nos diz quais palavras são mais plausíveis. Por exemplo, se começamos com uma distribuição uniforme sobre as sentenças mostradas no painel superior da figura 7.13, vemos que a primeira palavra é esperar em dois terços das sentenças e fugir no outro terço. Isso significa que na primeira etapa de tempo do primeiro POMDP de baixo nível, nosso vetor D deve atribuir essas probabilidades a essas palavras.</p>
<p>As palavras no nível inferior geram observações, entradas visuais com base em qual parte da palavra está atualmente <foveated> . Assim como no exemplo da figura 7.9, o POMDP permite a seleção de diferentes alvos foveais para acumular evidências a favor ou contra cada palavra hipotética. Isso apela para os mesmos processos de minimização de energia livre esperados descritos acima; portanto, não detalharemos as <foveal> específicas feitas aqui, mas notamos que a cada passo de tempo no nível inferior, há um aumento na confiança sobre a palavra em jogo. Na sequência mostrada na figura 7.13, vemos que a evidência é acumulada para a palavra fugir no nível inferior ao longo dos primeiros passos de tempo (na escala rápida, τ (1)). Essa inferência é propagada de volta ao nível superior, onde fornece evidências para a primeira e a quarta sentenças (cada uma das quais começa com essa palavra). Ao longo das etapas de tempo subsequentes, a evidência acumulada no nível inferior é consistente com ambas as sentenças. No quarto passo (na escala lenta, τ (2)), prevemos esperar na primeira sentença e fugir na segunda. Ao inferir espera na escala de tempo rápida, a primeira sentença é inferida na escala lenta. Na etapa final, a simulação seleciona a frase correta e é recompensada com o feedback correto. A atualização de crença resultante é vista no gráfico LFP na parte inferior da figura 7.12.</p>
<div class="figure">
<img src="images/Figura_7_13.png" alt="" />
<p class="caption">Figura 7.13 A atualização de crenças ocorre em várias escalas de tempo invertendo um modelo de inferência hierárquica simulado. Isso se baseia em um modelo generativo com uma separação de escalas de tempo (mostrada como uma escala de tempo lenta—τ    (2)—e uma escala de tempo rápida—τ (1)). A atualização de crenças no nível superior (s(2) ), representando sentenças, é mais lenta do que no nível inferior (s(1)), representando palavras. Painel inferior: LFPs, ou seja, a taxa de mudança das expectativas de log - que é proporcional aos erros de previsão (ε ) mostrados nas figuras anteriores.</p>
</div>
<p>Modelos temporais profundos desse tipo foram usados para simular leitura (Friston, Rosch et al. 2017), tarefas de memória de trabalho de período de atraso (Parr e Friston 2017c) e cálculo de priors empíricos para inferência visual (Parr, Benrimoh et al. 2018). Além disso, eles foram alavancados em explicações teóricas de motivação e controle (Pezzulo, Rigoli e Friston 2018). Em princípio, esses modelos podem ser estendidos a um número arbitrário de níveis, representando um mundo profundamente estruturado com dinâmicas que se desenrolam em muitas escalas temporais diferentes.</p>
<p>Podemos traçar um paralelo interessante entre os modelos hierárquicos do tipo das figuras 7.12 e 7.13 e os modelos de aprendizagem do tipo da figura 7.10. Modelos de aprendizagem podem ser considerados modelos generativos hierárquicos, que destacam uma separação de escalas de tempo entre dinâmicas inferenciais mais rápidas (atualizações de crenças sobre estados) e dinâmicas de aprendizagem mais lentas (atualizações de crenças sobre parâmetros). Os modelos mostrados nas figuras 7.10 e 7.12 também podem ser combinados a níveis arbitrários de complexidade, onde as próprias relações entre variáveis em diferentes níveis podem ser aprendidas. Isso permite projetar modelos generativos cada vez mais sofisticados que abordam questões cognitivas e neurobiológicas em nível de sistema.</p>
</div>
<div id="resumo-6" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Resumo<a href="inferência-ativa-em-tempo-discreto-1.html#resumo-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Neste capítulo, vimos algumas das maneiras pelas quais os modelos generativos de tempo discreto podem ser construídos para abordar uma série de problemas cognitivos e neurobiológicos, como inferência perceptual, tomada de decisão e planejamento, exploração e exploração de equilíbrio, aprendizado paramétrico e de estrutura , e busca de novidades. Isso está longe de ser um resumo exaustivo das aplicações de modelos discretos em Inferência Ativa, mas serve para ilustrar os princípios-chave desse tipo de modelagem. Os modelos descritos acima podem ser combinados hierarquicamente, com priorização adicionada sobre parâmetros e com priorização sensível ao contexto para políticas ou preferências. É importante ressaltar que a inferência usando modelos generativos simples e mais complexos sempre pode proceder através da minimização da energia livre, o que ilustra a generalidade da abordagem. O fato de que diferentes aspectos da Inferência Ativa se tornam aparentes sob modelos generativos distintos (por exemplo, busca de novidades com prioris sobre os parâmetros do modelo) abre a possibilidade de explorar um conjunto aberto de problemas cognitivos e biológicos projetando os modelos generativos apropriados.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="uma-receita-para-projetar-modelos-de-inferência-ativos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inferência-ativa-em-tempo-contínuo-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Inferencia Ativa.pdf", "Inferencia Ativa.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
